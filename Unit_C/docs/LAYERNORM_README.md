# nn.Parameter å’Œ nn.LayerNorm C å®ç°

## â“ é—®é¢˜

**nn.Parameter å’Œ nn.LayerNorm å¯ä»¥ç”¨ C è¯­è¨€å®ç°å—ï¼Ÿ**

## âœ… ç­”æ¡ˆ

**æ˜¯çš„ï¼å®Œå…¨å¯ä»¥ï¼**

## ğŸ“¦ å·²åˆ›å»ºæ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| [layernorm.h](layernorm.h) | å¤´æ–‡ä»¶ |
| [layernorm.c](layernorm.c) | **å®Œæ•´å®ç°** |
| [test_layernorm.c](test_layernorm.c) | æµ‹è¯•ç¨‹åºï¼ˆ6ä¸ªæµ‹è¯•ï¼‰ |
| [Makefile_layernorm](Makefile_layernorm) | ç¼–è¯‘é…ç½® |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### Windows

```batch
cd Unit_C
gcc -Wall -O2 -std=c99 -c conv2d.c
gcc -Wall -O2 -std=c99 -c layernorm.c
gcc -Wall -O2 -std=c99 -c test_layernorm.c
gcc conv2d.o layernorm.o test_layernorm.o -o test_layernorm.exe -lm
test_layernorm.exe
```

### Linux/Mac

```bash
cd Unit_C
make -f Makefile_layernorm run
```

## ğŸ“‹ å®ç°å†…å®¹

### 1. nn.Parameter

**åœ¨ C ä¸­çš„å®ç°**ï¼šå°±æ˜¯æ™®é€šçš„ float æ•°ç»„

```c
// PyTorch
self.weight = nn.Parameter(torch.randn(10, 20))

// C å®ç°
Parameter* weight = parameter_create(shape, ndim);
// æˆ–è€…ç›´æ¥
float* weight = (float*)malloc(10 * 20 * sizeof(float));
```

**ç‰¹ç‚¹**ï¼š
- åœ¨ PyTorch ä¸­æ˜¯å¯å­¦ä¹ çš„å¼ é‡
- åœ¨ C ä¸­å°±æ˜¯æ™®é€šæ•°ç»„
- ä»æ¨¡å‹æ–‡ä»¶åŠ è½½
- æ¨ç†æ—¶ä¿æŒä¸å˜

### 2. nn.LayerNorm

**å…¬å¼**ï¼š
```
mean = mean(x, dim=normalized_dims)
var = var(x, dim=normalized_dims)
y = gamma * (x - mean) / sqrt(var + eps) + beta
```

**C å®ç°**ï¼š
```c
// åˆ›å»º LayerNorm
int normalized_shape[] = {97, 16};  // (width, hidden_size)
LayerNormParams* ln = layernorm_create(
    normalized_shape,  // å½’ä¸€åŒ–çš„ç»´åº¦
    2,                 // ndim
    gamma,             // ç¼©æ”¾å‚æ•°ï¼ˆå¯ä¸º NULLï¼Œé»˜è®¤ 1ï¼‰
    beta,              // åç§»å‚æ•°ï¼ˆå¯ä¸º NULLï¼Œé»˜è®¤ 0ï¼‰
    1e-8f              // eps
);

// å‰å‘ä¼ æ’­
layernorm_forward_4d(input, ln);  // 4D å¼ é‡ç‰ˆæœ¬

// æ¸…ç†
layernorm_free(ln);
```

## ğŸ¯ GTCRN ä¸­çš„ä½¿ç”¨

### DPGRNN æ¨¡å—ï¼ˆlines 186-225ï¼‰

```python
class DPGRNN(nn.Module):
    def __init__(self, input_size, width, hidden_size):
        super().__init__()
        # ...
        self.intra_ln = nn.LayerNorm((width, hidden_size), eps=1e-8)
        self.inter_ln = nn.LayerNorm((width, hidden_size), eps=1e-8)
```

**ä½œç”¨**ï¼š
- ç¨³å®š RNN è®­ç»ƒ
- å½’ä¸€åŒ– (width, hidden_size) ç»´åº¦
- é…åˆæ®‹å·®è¿æ¥ä½¿ç”¨

### C å®ç°

```c
// DPGRNN é…ç½®
int width = 97;
int hidden_size = 16;

// åˆ›å»º LayerNorm
int normalized_shape[] = {width, hidden_size};
LayerNormParams* intra_ln = layernorm_create(
    normalized_shape, 2, NULL, NULL, 1e-8f
);
LayerNormParams* inter_ln = layernorm_create(
    normalized_shape, 2, NULL, NULL, 1e-8f
);

// Intra RNN ååº”ç”¨
// input: (B, T, F, C) å…¶ä¸­ F=width, C=hidden_size
layernorm_forward_4d(intra_output, intra_ln);

// Inter RNN ååº”ç”¨
layernorm_forward_4d(inter_output, inter_ln);
```

## ğŸ’¡ LayerNorm vs BatchNorm

### BatchNorm2d

```
å½’ä¸€åŒ–ç»´åº¦: å¯¹æ¯ä¸ªé€šé“ï¼Œåœ¨ batch å’Œç©ºé—´ç»´åº¦ä¸Š
ç»Ÿè®¡é‡: è·¨ batch è®¡ç®—
è¾“å…¥: (B, C, H, W)
å½’ä¸€åŒ–: (B, H, W) å¯¹æ¯ä¸ª C
å‚æ•°: gamma[C], beta[C]
ç”¨é€”: CNN
```

### LayerNorm

```
å½’ä¸€åŒ–ç»´åº¦: å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œåœ¨ç‰¹å¾ç»´åº¦ä¸Š
ç»Ÿè®¡é‡: æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹
è¾“å…¥: (B, ..., normalized_dims)
å½’ä¸€åŒ–: normalized_dims
å‚æ•°: gamma[normalized_dims], beta[normalized_dims]
ç”¨é€”: RNN/Transformer
```

### GTCRN ä½¿ç”¨åœºæ™¯

| æ¨¡å— | å½’ä¸€åŒ–ç±»å‹ | åŸå›  |
|------|-----------|------|
| ConvBlock | BatchNorm2d | CNN å±‚ï¼Œæœ‰è¶³å¤Ÿçš„ batch |
| GTConvBlock | BatchNorm2d | CNN å±‚ |
| DPGRNN | LayerNorm | RNN å±‚ï¼Œä¸ä¾èµ– batch |

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€ LayerNorm

```c
// è¾“å…¥: (batch_size, num_features)
int batch_size = 4;
int num_features = 10;

float* input = (float*)malloc(batch_size * num_features * sizeof(float));
float* output = (float*)malloc(batch_size * num_features * sizeof(float));

// åˆ›å»º LayerNorm
int normalized_shape[] = {num_features};
LayerNormParams* ln = layernorm_create(
    normalized_shape, 1, NULL, NULL, 1e-5f
);

// å‰å‘ä¼ æ’­
layernorm_forward(input, output, batch_size, ln);

// æ¸…ç†
free(input);
free(output);
layernorm_free(ln);
```

### ç¤ºä¾‹ 2: GTCRN DPGRNN

```c
// DPGRNN è¾“å…¥: (B, T, F, C)
int batch = 1;
int time_steps = 63;
int width = 97;
int hidden_size = 16;

Tensor* input = tensor_create(batch, time_steps, width, hidden_size);

// åˆ›å»º LayerNorm
int normalized_shape[] = {width, hidden_size};
LayerNormParams* intra_ln = layernorm_create(
    normalized_shape, 2, NULL, NULL, 1e-8f
);

// Intra RNN å¤„ç†
// ... (RNN å‰å‘ä¼ æ’­)

// åº”ç”¨ LayerNorm
layernorm_forward_4d(input, intra_ln);

// æ®‹å·®è¿æ¥
// output = input + intra_output

// æ¸…ç†
layernorm_free(intra_ln);
tensor_free(input);
```

### ç¤ºä¾‹ 3: å¯å­¦ä¹ å‚æ•°

```c
// è‡ªå®šä¹‰ gamma å’Œ beta
int num_features = 10;
float* gamma = (float*)malloc(num_features * sizeof(float));
float* beta = (float*)malloc(num_features * sizeof(float));

// ä»æ¨¡å‹æ–‡ä»¶åŠ è½½
// load_from_file(gamma, "gamma.bin");
// load_from_file(beta, "beta.bin");

// æˆ–æ‰‹åŠ¨è®¾ç½®
for (int i = 0; i < num_features; i++) {
    gamma[i] = 1.0f;  // ç¼©æ”¾
    beta[i] = 0.0f;   // åç§»
}

// åˆ›å»º LayerNorm
int normalized_shape[] = {num_features};
LayerNormParams* ln = layernorm_create(
    normalized_shape, 1, gamma, beta, 1e-5f
);

// ä½¿ç”¨
layernorm_forward(input, output, batch_size, ln);

// æ¸…ç†
free(gamma);
free(beta);
layernorm_free(ln);
```

## ğŸ” å®ç°ç»†èŠ‚

### LayerNorm ç®—æ³•

```c
void layernorm_forward(
    float* input,
    float* output,
    int batch_size,
    const LayerNormParams* params
) {
    int num_features = params->num_features;

    // å¯¹æ¯ä¸ªæ ·æœ¬
    for (int b = 0; b < batch_size; b++) {
        // 1. è®¡ç®—å‡å€¼
        float sum = 0.0f;
        for (int i = 0; i < num_features; i++) {
            sum += input[b * num_features + i];
        }
        float mean = sum / num_features;

        // 2. è®¡ç®—æ–¹å·®
        float var_sum = 0.0f;
        for (int i = 0; i < num_features; i++) {
            float diff = input[b * num_features + i] - mean;
            var_sum += diff * diff;
        }
        float var = var_sum / num_features;

        // 3. å½’ä¸€åŒ–
        float std = sqrt(var + params->eps);
        for (int i = 0; i < num_features; i++) {
            float normalized = (input[b * num_features + i] - mean) / std;
            output[b * num_features + i] =
                params->gamma[i] * normalized + params->beta[i];
        }
    }
}
```

### 4D å¼ é‡ç‰ˆæœ¬

```c
void layernorm_forward_4d(
    Tensor* input,  // (B, T, F, C)
    const LayerNormParams* params
) {
    // å°† 4D å¼ é‡è§†ä¸º (B*T, F*C)
    int batch_size = input->shape.batch * input->shape.channels;
    int num_features = input->shape.height * input->shape.width;

    // è°ƒç”¨æ ‡å‡† LayerNorm
    layernorm_forward(input->data, input->data, batch_size, params);
}
```

## ğŸ“Š æ€§èƒ½

| æ“ä½œ | å¤æ‚åº¦ | å†…å­˜ |
|------|--------|------|
| LayerNorm | O(B Ã— N) | In-place æˆ–è¾“å‡ºç¼“å†² |
| Parameter | O(1) | å‚æ•°å¤§å° |

å…¶ä¸­ N = num_featuresï¼ˆå½’ä¸€åŒ–ç»´åº¦çš„ä¹˜ç§¯ï¼‰

## âš ï¸ æ³¨æ„äº‹é¡¹

### LayerNorm

1. **å½’ä¸€åŒ–ç»´åº¦**ï¼šå¿…é¡»ä¸è¾“å…¥åŒ¹é…
2. **eps å€¼**ï¼šGTCRN ä½¿ç”¨ 1e-8ï¼ˆæ¯” BatchNorm çš„ 1e-5 æ›´å°ï¼‰
3. **In-place æ“ä½œ**ï¼š`layernorm_forward_4d` ç›´æ¥ä¿®æ”¹è¾“å…¥
4. **æ•°å€¼ç¨³å®šæ€§**ï¼šä½¿ç”¨ double ç´¯åŠ é¿å…ç²¾åº¦æŸå¤±

### Parameter

1. **å†…å­˜ç®¡ç†**ï¼šéœ€è¦æ‰‹åŠ¨ malloc/free
2. **æ¨¡å‹åŠ è½½**ï¼šä»æ–‡ä»¶è¯»å–è®­ç»ƒå¥½çš„å‚æ•°
3. **æ¨ç†æ¨¡å¼**ï¼šå‚æ•°ä¿æŒä¸å˜
4. **åˆå§‹åŒ–**ï¼šè®­ç»ƒæ—¶éœ€è¦åˆé€‚çš„åˆå§‹åŒ–ç­–ç•¥

## ğŸ“ˆ GTCRN DPGRNN å®Œæ•´æµç¨‹

```c
// 1. åˆ›å»ºå‚æ•°
int width = 97;
int hidden_size = 16;

int normalized_shape[] = {width, hidden_size};
LayerNormParams* intra_ln = layernorm_create(
    normalized_shape, 2, NULL, NULL, 1e-8f
);
LayerNormParams* inter_ln = layernorm_create(
    normalized_shape, 2, NULL, NULL, 1e-8f
);

// 2. Intra RNN
// input: (B, C, T, F) -> permute -> (B, T, F, C)
// reshape -> (B*T, F, C)
// intra_rnn(input) -> (B*T, F, C)
// intra_fc(input) -> (B*T, F, C)
// reshape -> (B, T, F, C)

layernorm_forward_4d(intra_output, intra_ln);

// æ®‹å·®è¿æ¥
// intra_out = input + intra_output

// 3. Inter RNN
// intra_out: (B, T, F, C) -> permute -> (B, F, T, C)
// reshape -> (B*F, T, C)
// inter_rnn(input) -> (B*F, T, C)
// inter_fc(input) -> (B*F, T, C)
// reshape -> (B, F, T, C)
// permute -> (B, T, F, C)

layernorm_forward_4d(inter_output, inter_ln);

// æ®‹å·®è¿æ¥
// inter_out = intra_out + inter_output

// 4. è¾“å‡º
// inter_out: (B, T, F, C) -> permute -> (B, C, T, F)
```

## ğŸ“š æµ‹è¯•

ç¨‹åºè¿è¡Œ **6 ä¸ªæµ‹è¯•**ï¼š

1. **Test 1**: nn.Parameter - å‚æ•°ç®¡ç†
2. **Test 2**: LayerNorm åŸºç¡€æµ‹è¯•
3. **Test 3**: LayerNorm 2D å½’ä¸€åŒ–
4. **Test 4**: GTCRN DPGRNN LayerNorm
5. **Test 5**: LayerNorm vs BatchNorm å¯¹æ¯”
6. **Test 6**: å¯å­¦ä¹ å‚æ•°

## âœ… æ€»ç»“

### é—®é¢˜
nn.Parameter å’Œ nn.LayerNorm å¯ä»¥ç”¨ C å®ç°å—ï¼Ÿ

### ç­”æ¡ˆ
**æ˜¯çš„ï¼å®Œå…¨å¯ä»¥ï¼**

### nn.Parameter
- âœ… å°±æ˜¯æ™®é€š float æ•°ç»„
- âœ… æ‰‹åŠ¨å†…å­˜ç®¡ç†
- âœ… ä»æ¨¡å‹æ–‡ä»¶åŠ è½½
- âœ… æ¨ç†æ—¶ä¸å˜

### nn.LayerNorm
- âœ… å½’ä¸€åŒ–æŒ‡å®šç»´åº¦
- âœ… æ¯æ ·æœ¬ç‹¬ç«‹ç»Ÿè®¡
- âœ… ä¸ä¾èµ– batch
- âœ… é€‚åˆ RNN

### GTCRN ä½¿ç”¨
- DPGRNN intra_ln
- DPGRNN inter_ln
- å½’ä¸€åŒ– (width, hidden_size)
- é…åˆæ®‹å·®è¿æ¥

### è¿è¡Œ
```bash
make -f Makefile_layernorm run
```

---

**åˆ›å»ºæ—¶é—´**: 2025-12-18
**è¯­è¨€**: C99
**çŠ¶æ€**: âœ… å®Œæˆå¹¶æµ‹è¯•
